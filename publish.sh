#!/bin/bash
set -e

# ============================================================================
# CONFIGURACIÃ“N DEL STACK
# ============================================================================
BIGDATA_BASE="$HOME/Documentos/docker-compose-contenedores/bigdata/volumenes"
BIGDATA_SHARED="$BIGDATA_BASE/shared"
# Landing Zone donde Spark ve los archivos como locales
BIGDATA_MINIO_SHARE="$BIGDATA_SHARED/minioshareddata" 
PROJECT_NAME="fraud_pipeline"

# Colores
GREEN='\033[0;32m'
NC='\033[0m'
print_ok() { echo -e "${GREEN}[âœ“]${NC} $1"; }

# ============================================================================
# 1. INFRAESTRUCTURA
# ============================================================================
print_ok "âš™ï¸ Preparando directorios en Share..."
# Creamos la carpeta de aterrizaje (Landing)
sudo mkdir -p "$BIGDATA_MINIO_SHARE/landing/payments"

# Permisos generales para el volumen compartido
sudo chmod -R 777 "$BIGDATA_SHARED"
sudo chown -R 1000:1000 "$BIGDATA_SHARED"

# ============================================================================
# 2. GENERACIÃ“N DE DATOS
# ============================================================================
print_ok "âš¡ Ejecutando Generador de Datos..."

# âœ… CORRECCIÃ“N: Ejecutamos el script desde su ubicaciÃ³n real 'data/'
# Como se ejecuta desde la raÃ­z, el output irÃ¡ a 'data/input_events' correctamente
python3 data/raw_generator.py

print_ok "ğŸ“¦ Moviendo datos a Landing Zone..."
# Sincronizamos desde la carpeta local data/input_events hacia el Share
if [ -d "data/input_events" ]; then
    rsync -av --delete data/input_events/ "$BIGDATA_MINIO_SHARE/landing/payments/"
fi

# Aseguramos permisos (UID 1000) para que Spark pueda leer
sudo chown -R 1000:1000 "$BIGDATA_MINIO_SHARE/landing"
sudo chmod -R 777 "$BIGDATA_MINIO_SHARE/landing"

# ============================================================================
# 3. PUBLICACIÃ“N DE CÃ“DIGO
# ============================================================================
print_ok "ğŸ“ Publicando Scripts y DAGs..."

# Scripts de Spark
TARGET_SPARK="$BIGDATA_SHARED/scripts_airflow/$PROJECT_NAME"
mkdir -p "$TARGET_SPARK"
# Tu tree muestra 'spark_jobs' en la raÃ­z, esto es correcto:
rsync -av --delete spark_jobs/ "$TARGET_SPARK/"

# DAGs de Airflow
TARGET_DAGS="$BIGDATA_SHARED/dags_airflow"
mkdir -p "$TARGET_DAGS"
# Tu tree muestra 'dags' en la raÃ­z, esto es correcto:
rsync -av dags/ "$TARGET_DAGS/"

# Permisos de lectura finales
find "$BIGDATA_SHARED" -type f -name "*.py" -exec chmod 666 {} \; 2>/dev/null || true

print_ok "ğŸš€ Pipeline desplegado correctamente."